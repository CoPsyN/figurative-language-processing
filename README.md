# The Model Behavior of Large Language Models in Figurative Language Processing

## About

This repository contains all source code and data for the following (forthcoming) paper: 

Hyewon Jang<sup><b>&#8224;</b></sup>, Qi Yu<sup><b>&#8224;</b></sup> and Diego Frassinelli. 
The Inner Workings of Language Models in Figurative Language Processing: Linguistically Informed Feature Analyses of Model Behavior.

<sup><b>&#8224;</b></sup> Authors with equal contribution, listed alphabetically.

## A short summary of the paper

This paper aims at investigating the behavior of Transformer-based language models (TLMs) in the task of figurative language processing.
We conducted feature importance analyses for 3 Transformer-based language models (BERT, RoBERTa, XLM), 
and compared their behaviors with two white-box models (Logistic Regression, Random Forest).


