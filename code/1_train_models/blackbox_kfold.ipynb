{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aee36b0-2c7f-4b8d-a38b-742c16cf0c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "_CudaDeviceProperties(name='NVIDIA A100-PCIE-40GB', major=8, minor=0, total_memory=40370MB, multi_processor_count=108)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_properties(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea856553-e465-4e63-86e6-7650ec7309b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 23:27:53.364566: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import random, time, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification \n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from matplotlib import pyplot as plt\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf2e1d69-cb72-43f4-a755-9a9f600cd794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10848, 4)\n",
      "0    6506\n",
      "1    2212\n",
      "2     884\n",
      "3     625\n",
      "4     621\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_binary</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I can't believe my ex didn't pay his car note ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sarcasm_premise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>But then the paper would not find out about yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Idiom_premise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Last week my kid said some really mean things ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CreativeParaphrase_premise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The gravy was so fatty, it made the meat taste...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Metaphor_premise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He pulls a giant disc out and flashes it like ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Simile_hypothesis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  label_binary  \\\n",
       "0  I can't believe my ex didn't pay his car note ...      0             0   \n",
       "1  But then the paper would not find out about yo...      0             0   \n",
       "2  Last week my kid said some really mean things ...      0             0   \n",
       "3  The gravy was so fatty, it made the meat taste...      0             0   \n",
       "4  He pulls a giant disc out and flashes it like ...      3             1   \n",
       "\n",
       "                       source  \n",
       "0             Sarcasm_premise  \n",
       "1               Idiom_premise  \n",
       "2  CreativeParaphrase_premise  \n",
       "3            Metaphor_premise  \n",
       "4           Simile_hypothesis  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/figlang_all.tsv\", sep=\"\\t\", encoding=\"utf-8\")\n",
    "print(df.shape)\n",
    "print(df[\"label\"].value_counts())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af837b9b-b965-45c8-b98f-20f749e0bd0a",
   "metadata": {},
   "source": [
    "# 1. Set the model to use\n",
    "\n",
    "**Uncomment one of the lines below to set the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b369096a-26e6-40ad-a24a-ab4d5156d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_NAME = \"bert-base-uncased\" \n",
    "#MODEL_NAME = \"roberta-base\" \n",
    "MODEL_NAME = \"xlnet-base-cased\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc635bea-57c6-44c9-a6ec-6e2dbd318edd",
   "metadata": {},
   "source": [
    "# 2. K-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27c1234c-b264-4b97-8b68-a319ceb7f4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_107653/3480551872.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_figlang[\"label\"] = df_figlang[\"label\"].map(label_map)\n"
     ]
    }
   ],
   "source": [
    "df_figlang = df.loc[df[\"label\"] != 0]\n",
    "\n",
    "# Re-map the label to start from 0, otherwise the training does not work:\n",
    "label_map = {1: 0,\n",
    "             2: 1,\n",
    "             3: 2,\n",
    "             4: 3}\n",
    "\n",
    "df_figlang[\"label\"] = df_figlang[\"label\"].map(label_map)\n",
    "\n",
    "X = df_figlang[\"text\"].values\n",
    "y = df_figlang[\"label\"].values\n",
    "\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7905fba3-8129-44b8-a74f-3d279ef53cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetTokenizerFast(name_or_path='xlnet-base-cased', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '<sep>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False), 'additional_special_tokens': ['<eop>', '<eod>']})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b374eb8f-099e-4e7f-8dce-a949ce6ddcb7",
   "metadata": {},
   "source": [
    "**Run a sample tokenization on all sequences in the training set to get the max_len:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4523718-d21f-4745-9973-58f869503157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  59\n"
     ]
    }
   ],
   "source": [
    "test_max_len = 0\n",
    "\n",
    "for text in df[\"text\"]:\n",
    "    input_ids = tokenizer.encode(text, \n",
    "                                 add_special_tokens=True)\n",
    "    test_max_len = max(test_max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', test_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bed7d10e-67a2-47d0-9f89-2db8712d9605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximal length of input sequence\n",
    "MAX_LEN = test_max_len\n",
    "\n",
    "# Number of labels \n",
    "NUM_LABELS = 4\n",
    "\n",
    "# Specifying batch size: For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 16 \n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98a79bd6-05c9-4225-b4a7-9ef0fb092fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with tokenization.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every text...\n",
    "for idx, text in enumerate(X):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,                      # Sentence to encode.\n",
    "        add_special_tokens = True, \n",
    "        padding = \"max_length\",\n",
    "        truncation = True,\n",
    "        max_length = MAX_LEN,\n",
    "        return_attention_mask = True,\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    # ----- Qi: Only for checking the results of tokenization\n",
    "    #print(text)\n",
    "    #print(tokenizer.convert_ids_to_tokens(encoded_dict['input_ids'][0])) \n",
    "    #print(len(tokenizer.convert_ids_to_tokens(encoded_dict['input_ids'][0])))\n",
    "    #print(\"-----\") \n",
    "    \n",
    "print(\"Done with tokenization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f40af908-3043-420c-b202-13f133536569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bc2c69c-925a-4fc3-bfb5-2da3d70957a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Helper function for formatting elapsed times.\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "814d5fa4-d34f-4e07-819c-fb4d01048c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLIT = 10\n",
    "kf = KFold(n_splits=N_SPLIT, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b3ac376-0f53-49bd-8bb1-0a27bb65d80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/jupyterhub/lib64/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ======== Epoch 1 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:06.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:11.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:16.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:21.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:26.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:31.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epoch took: 0:00:32\n",
      "\n",
      "  ======== Epoch 2 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "  ======== Epoch 3 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:24.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:29.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:00:30\n",
      "\n",
      "  ======== Epoch 4 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "  Running Test...\n",
      "  Predicting labels for 435 test sentences...\n",
      "  F1 of fold 0: 0.9320957177\n",
      "\n",
      "FOLD 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/jupyterhub/lib64/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ======== Epoch 1 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "  ======== Epoch 2 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epoch took: 0:00:30\n",
      "\n",
      "  ======== Epoch 3 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "  ======== Epoch 4 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:00:30\n",
      "\n",
      "  Running Test...\n",
      "  Predicting labels for 435 test sentences...\n",
      "  F1 of fold 1: 0.942476331\n",
      "\n",
      "FOLD 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/jupyterhub/lib64/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ======== Epoch 1 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "  ======== Epoch 2 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:06.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:12.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:17.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:23.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:28.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:33.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epoch took: 0:00:33\n",
      "\n",
      "  ======== Epoch 3 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:26.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "  ======== Epoch 4 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "  Running Test...\n",
      "  Predicting labels for 434 test sentences...\n",
      "  F1 of fold 2: 0.9454106896\n",
      "\n",
      "FOLD 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/jupyterhub/lib64/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ======== Epoch 1 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:16.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:21.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:27.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:34.\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epoch took: 0:00:35\n",
      "\n",
      "  ======== Epoch 2 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:11.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:16.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:21.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:26.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "  ======== Epoch 3 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:31.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "  ======== Epoch 4 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:16.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:21.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:27.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:32.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:00:33\n",
      "\n",
      "  Running Test...\n",
      "  Predicting labels for 434 test sentences...\n",
      "  F1 of fold 3: 0.9640030225\n",
      "\n",
      "FOLD 5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/jupyterhub/lib64/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ======== Epoch 1 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.39\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "  ======== Epoch 2 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:00:30\n",
      "\n",
      "  ======== Epoch 3 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "  ======== Epoch 4 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:16.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:21.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:26.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:31.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:00:32\n",
      "\n",
      "  Running Test...\n",
      "  Predicting labels for 434 test sentences...\n",
      "  F1 of fold 4: 0.9602824974\n",
      "\n",
      "FOLD 6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/jupyterhub/lib64/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ======== Epoch 1 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:29.\n",
      "\n",
      "  Average training loss: 0.42\n",
      "  Training epoch took: 0:00:30\n",
      "\n",
      "  ======== Epoch 2 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "  ======== Epoch 3 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:31.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "  ======== Epoch 4 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "  Running Test...\n",
      "  Predicting labels for 434 test sentences...\n",
      "  F1 of fold 5: 0.9473156407\n",
      "\n",
      "FOLD 7:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/jupyterhub/lib64/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ======== Epoch 1 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "  ======== Epoch 2 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:16.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "  ======== Epoch 3 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:29.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:00:30\n",
      "\n",
      "  ======== Epoch 4 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:29.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:00:30\n",
      "\n",
      "  Running Test...\n",
      "  Predicting labels for 434 test sentences...\n",
      "  F1 of fold 6: 0.9259324094\n",
      "\n",
      "FOLD 8:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/jupyterhub/lib64/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ======== Epoch 1 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:16.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:22.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:28.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:33.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:00:33\n",
      "\n",
      "  ======== Epoch 2 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:06.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:11.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:17.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:22.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:28.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:33.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epoch took: 0:00:34\n",
      "\n",
      "  ======== Epoch 3 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:24.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:29.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:00:30\n",
      "\n",
      "  ======== Epoch 4 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:00:30\n",
      "\n",
      "  Running Test...\n",
      "  Predicting labels for 434 test sentences...\n",
      "  F1 of fold 7: 0.934771614\n",
      "\n",
      "FOLD 9:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/jupyterhub/lib64/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ======== Epoch 1 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:19.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:24.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:29.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:00:30\n",
      "\n",
      "  ======== Epoch 2 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epoch took: 0:00:30\n",
      "\n",
      "  ======== Epoch 3 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "  ======== Epoch 4 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:21.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:26.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:31.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:00:31\n",
      "\n",
      "  Running Test...\n",
      "  Predicting labels for 434 test sentences...\n",
      "  F1 of fold 8: 0.9518829567\n",
      "\n",
      "FOLD 10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/jupyterhub/lib64/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ======== Epoch 1 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:24.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:29.\n",
      "\n",
      "  Average training loss: 0.39\n",
      "  Training epoch took: 0:00:30\n",
      "\n",
      "  ======== Epoch 2 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:19.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:24.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:29.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epoch took: 0:00:30\n",
      "\n",
      "  ======== Epoch 3 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:06.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:11.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:17.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:22.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:27.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:32.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:00:32\n",
      "\n",
      "  ======== Epoch 4 / 4 ========\n",
      "  Training...\n",
      "    Batch    40  of    245.    Elapsed: 0:00:05.\n",
      "    Batch    80  of    245.    Elapsed: 0:00:10.\n",
      "    Batch   120  of    245.    Elapsed: 0:00:15.\n",
      "    Batch   160  of    245.    Elapsed: 0:00:20.\n",
      "    Batch   200  of    245.    Elapsed: 0:00:25.\n",
      "    Batch   240  of    245.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:00:30\n",
      "\n",
      "  Running Test...\n",
      "  Predicting labels for 434 test sentences...\n",
      "  F1 of fold 9: 0.9532180864\n",
      "\n",
      "Done.\n",
      "Average Macro-F1 (10 folds):  0.945738896544862\n"
     ]
    }
   ],
   "source": [
    "all_metrics = []\n",
    "\n",
    "for kf_idx, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "    print(\"FOLD {}:\".format(kf_idx+1))\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        num_labels = NUM_LABELS, \n",
    "        output_attentions = False, \n",
    "        output_hidden_states = True,\n",
    "    )\n",
    "    \n",
    "    # Tell pytorch to run this model on the GPU.\n",
    "    model.cuda()\n",
    "    \n",
    "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "    # I believe the 'W' stands for 'Weight Decay fix\"\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr = 2e-5,   # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                      eps = 1e-8   # args.adam_epsilon  - default is 1e-8.\n",
    "                     )    \n",
    "    \n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('  ======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('  Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Put the model into training mode. Don't be mislead--the call to `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training vs. test \n",
    "        # (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "        \n",
    "        # Create the DataLoader for our training set.\n",
    "        train_data = TensorDataset(input_ids[train_idx], attention_masks[train_idx], labels[train_idx])\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "        \n",
    "        # Total number of training steps is number of batches * number of epochs.\n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "        # Create the learning rate scheduler.\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                    num_training_steps = total_steps)\n",
    "        \n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            \n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "                # Report progress.\n",
    "                print('    Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "                \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)  \n",
    "            \n",
    "            # Always clear any previously calculated gradients before performing a backward pass. \n",
    "            # PyTorch doesn't do this automatically because accumulating the gradients is \"convenient while training RNNs\". \n",
    "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "            model.zero_grad()  \n",
    "            \n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # This will return the loss (rather than the model output) because we have provided the `labels`.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                            #token_type_ids=None, \n",
    "                            attention_mask=b_input_mask, \n",
    "                            labels=b_labels)\n",
    "            \n",
    "            # The call to `model` always returns a tuple, so we need to pull the loss value out of the tuple.\n",
    "            loss = outputs[0]\n",
    "            \n",
    "            # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end. \n",
    "            # `loss` is a Tensor containing a single value; the `.item()` function just returns the Python value from the tensor.\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            # The optimizer dictates the \"update rule\"--how the parameters are modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "    \n",
    "        # Calculate the average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)  \n",
    "        \n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))    \n",
    "        \n",
    "        \n",
    "    # ========================================\n",
    "    #               Test\n",
    "    # ========================================\n",
    "        \n",
    "    print(\"\")\n",
    "    print(\"  Running Test...\")\n",
    "        \n",
    "    # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
    "    model.eval()\n",
    "        \n",
    "    # Create the DataLoader.\n",
    "    prediction_data = TensorDataset(input_ids[test_idx], attention_masks[test_idx], labels[test_idx])\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "    \n",
    "    print('  Predicting labels for {:,} test sentences...'.format(len(input_ids[test_idx])))\n",
    "    \n",
    "    # Tracking variables \n",
    "    predictions , true_labels = [], []\n",
    "    \n",
    "    # Predict \n",
    "    for batch in prediction_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            outputs = model(b_input_ids, \n",
    "                            #token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "            \n",
    "        logits = outputs[0]\n",
    "        \n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Store predictions and true labels\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "    # Combine the results across all batches. \n",
    "    flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "    # For each sample, pick the label (0 or 1) with the higher score.\n",
    "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "    # Combine the correct labels for each batch into a single list.\n",
    "    flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "    \n",
    "    metric = f1_score(flat_true_labels, flat_predictions, average=\"macro\")\n",
    "    all_metrics.append(metric)\n",
    "    print(\"  F1 of fold {}:\".format(kf_idx),  round(metric, 10))\n",
    "    print()\n",
    "    \n",
    "print(\"Done.\")\n",
    "print(\"Average Macro-F1 ({} folds): \".format(kf_idx+1), mean(all_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02a553a8-4e13-43cb-a285-fd3c73e1624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../results/classification_results/\" + str(N_SPLIT) + \"folds_\" + MODEL_NAME + \".txt\", \"w\")\n",
    "\n",
    "for i in range(N_SPLIT):\n",
    "    f.write(\"Fold \" + str(i+1) + \": \" + str(all_metrics[i]) + \"\\n\")\n",
    "\n",
    "f.write(\"\\nAVERAGE MACRO-F1 of \" + str(N_SPLIT) + \"FOLDS: \" + str(mean(all_metrics)))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
